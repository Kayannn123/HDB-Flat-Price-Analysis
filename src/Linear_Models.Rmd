---
title: "Linear Models"
author: "Jiaxin Zheng A0266129Y"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
library(dplyr)
library(ggplot2)
library(lubridate)
library(readr)
library(stringr)
library(ggpubr)
library(corrplot)
library(car)
library(MASS)
library(splines)
library(boot)
library(mgcv)
```

For linear models, R will automatically deal with categorical variable. However, for numerical values, we try to scale them to avoid the dominant affect of those with high numeric values. 

```{r}
df_clean <- read.csv("../data/df_clean.csv")
head(df_clean)
df_clean <- df_clean[,2:11]
```
```{r}
df_clean %>%
  mutate(month = ymd(month), town = as.factor(town), flat_type = as.factor(flat_type),
         flat_model_group = as.factor(flat_model_group), region = as.factor(region)) %>%
  mutate(across(c(floor_area_sqm, remaining_lease_years, storey_median), scale)) -> df_linear

df_linear <- df_linear[, !(names(df_linear) %in% c("lease_commence_date", "town"))]
head(df_linear)
```


### Test_train split
```{r}
set.seed(4248)
n = length(df_linear$resale_price)
print(n)
trainidx = sample(n, 0.8*n)
train = df_linear[trainidx,]
test = df_linear[-trainidx,]
```
### Baseline Model
We first build our baseline model - simple linear regression model given its explanability and nice fit with numeric value for this regression problem. 
```{r}
lr_full <- lm(resale_price ~ ., data = train)
summary(lr_full)
```
```{r}
vif(lr_full)
```
We notice a high possibility in multicollinearity for variable `floor_area_sqm`. This is probably because thos with same `flat_type` would possibly share same range of floor_area_sqm. To deal with this question, we first investigate the distribution of `floor_area_sqm`  under each flat_type.
```{r}
flat_types <- unique(df_clean$flat_type)
ggplot(df_clean, aes(x = floor_area_sqm)) +
  geom_histogram(binwidth = 5, fill = "lightblue", color = "white") +
  facet_wrap(~flat_type, scales = "free_y") +
  labs(title = "Distribution of Floor Area by Flat Type", x = "Floor Area (sqm)", y = "Count")

```
Usually, both `flat_type` and `floor_area_sqm` would both contribute to a customer's decision. Therefore, we cannot simply drop it. Instead, we try to add an interaction term. We use F-test to see whether it's better to add interaction term or to simply drop `flat_type` or `floor_area_sqm`
```{r}
lr_interact <- lm(resale_price ~ floor_area_sqm * flat_type + remaining_lease_years +
                      flat_model_group + region + storey_median + month, data = train)
lr_sqm <- lm(resale_price ~ floor_area_sqm + remaining_lease_years + flat_model_group + region +storey_median+month, data = train)
lr_type <- lm(resale_price ~ flat_type + remaining_lease_years + flat_model_group + region +storey_median+month, data = train)

anova(lr_full, lr_sqm, lr_type, lr_interact)
```
Removing either variable significantly worsens model performance (F = 289.61, p < 2.2e-16 for `flat_type`; p < 2.2e-16 for `floor_area_sqm`). Furthermore, including the interaction term between `flat_type` and `floor_area_sqm` significantly improved the model (F = 3502.93, p < 2.2e-16).

```{r}
fitted_vals <- predict(lr_interact)
stud_resid <- rstudent(lr_interact)

resid_df <- data.frame(fitted = fitted_vals, stud_resid = stud_resid)
resid_df <- resid_df %>%
  mutate(is_outlier = abs(stud_resid) > 3)
ggplot(resid_df, aes(x = fitted, y = stud_resid)) +
  geom_point(aes(color = is_outlier), alpha = 0.3) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  geom_hline(yintercept = c(-3, 3), linetype = "dotted", color = "red") +
  scale_color_manual(values = c("black", "red")) +
  labs(
    title = "Studentized Residuals vs Predicted Resale Price",
    subtitle = "Red dots indicate potential outliers (|resid| > 3)",
    x = "Predicted Resale Price",
    y = "Studentized Residual",
    color = "Outlier"
  ) +
  theme_minimal(base_size = 13)

```
```{r}
subset(resid_df, stud_resid < -10)
extreme_outlier <- resid_df %>%
  filter(stud_resid < -10) %>%
  mutate(row_id = row_number())


index_to_drop <- which(resid_df$stud_resid < -10)
train[index_to_drop,]
train_cleaned <- train[-index_to_drop, ]

```
The studentised-residual plot indicates a generally stable distribution of studentized residuals, with most values within ±3, suggesting a good model fit. A few high-price transactions (>$1.5M) show larger residuals, potentially due to underfitting of rare flat types. The plot also exhibits mild heteroscedasticity, where variance increases with predicted values. This may motivate the use of `log(resale_price)` transformation or more flexible models such as splines or GAM.

#### Compared with the ls model without transformation
```{r}
fitted_price_base = predict(lr_full, newdata = test)
mse <- mean((test$resale_price - fitted_price_base)^2)
rmse <- sqrt(mse)
print(paste("RMSE on test set:", round(rmse, 2)))
print(paste("Adjusted_R^2:", round(summary(lr_interact)$adj.r.squared,2)))
```

### Refit the model with the interaction term
```{r}
lr.fit = lm(log(resale_price) ~ floor_area_sqm * flat_type + remaining_lease_years +
                      flat_model_group + region + storey_median + month, data = train)
fitted_price_log = predict(lr.fit, newdata = test)
mse <- mean((test$resale_price - exp(fitted_price_log))^2)
rmse <- sqrt(mse)
print(paste("RMSE on test set:", round(rmse, 2)))
print(paste("Adjusted_R^2:", round(summary(lr.fit)$adj.r.squared,2)))
```
### Residual plot for every variable
```{r}
predictors <- c("floor_area_sqm", "remaining_lease_years", "storey_median", "month")  
par(mfrow = c(2, 2))

for (var in predictors) {
  plot(train[[var]], residuals(lr.fit),
       main = paste("Residuals vs", var),
       xlab = var, ylab = "Residuals",
       pch = 20, col = rgb(0, 0, 0, 0.3))
  abline(h = 0, col = "red", lty = 2)
}
```
Residual plots were used to evaluate the linearity assumption for each predictor. While most variables such as remaining_lease_years and storey_median showed well-behaved, flat residual patterns, slight curvature was observed for floor_area_sqm and month, suggesting potential non-linear effects. We thus consider incorporating spline terms for these two variables in subsequent models.

### Natural Cubic Spline
```{r}
get_cv_error <- function(df_val) {
  model <- glm(log(resale_price) ~ ns(floor_area_sqm, df = df_val) +
                 remaining_lease_years + flat_type + flat_model_group +
                 region + storey_median + month,
               data = train)
  cv_result <- cv.glm(train, model, K = 10)
  return(cv_result$delta[1])
}

df_list <- 4:15
cv_errors <- sapply(df_list, get_cv_error)

plot(df_list, cv_errors, type = "b", pch = 19,
     xlab = "Degrees of Freedom for ns(floor_area_sqm)",
     ylab = "10-fold CV Error",
     main = "CV Error vs Spline Degrees of Freedom")
```
Adding more degrees of freedom would have the risk of overfitting, however, there is obvious trend of stable choice of df. We choose knot = 7 to avoid underfitting and overfitting.

```{r}
model_ns <- lm(log(resale_price) ~ ns(floor_area_sqm, df = 7) +
                 remaining_lease_years + flat_type + flat_model_group +
                 region + storey_median + month,
               data = train)

predict_ns <- predict(model_ns, newdata=test)
```

### Smoothing Spline
```{r}
model_gam <- gam(log(resale_price) ~ s(floor_area_sqm) +
                   remaining_lease_years + flat_type + flat_model_group +
                   region + storey_median + month,
                 data = train)
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

pred_log_gam <- predict(model_gam, newdata = test)
pred_gam <- exp(pred_log_gam)
rmse_gam <- rmse(test$resale_price, pred_gam)
```

```{r}
rmse_linear <- rmse(test$resale_price, exp(fitted_price_log))
rmse_nspline <- rmse(test$resale_price, exp(predict_ns))
rmse_gam     <- rmse(test$resale_price, pred_gam)

adj_r2_base    <- summary(lr.fit)$adj.r.squared
adj_r2_nspline <- summary(model_ns)$r.sq
adj_r2_gam     <- summary(model_gam)$r.sq  

result <- data.frame(
  Model = c("Linear", "Natural Spline", "GAM"),
  RMSE  = round(c(rmse_linear, rmse_nspline, rmse_gam), 2),
  Adjusted_R2 = round(c(adj_r2_base, adj_r2_nspline, adj_r2_gam), 4)
)


result
```

Although natural spline and GAM models achieved marginal improvements in RMSE and adjusted R² compared to the linear baseline, the gains were minimal. Given the significantly increased model complexity and computational cost, the linear model remains a strong, interpretable, and efficient choice for predicting resale prices.

### Model Interpretation
```{r}
coef(lr.fit)
coef(model_ns)
coef(model_gam)
```
### Hypothesis Testingn (Q2)
```{r}
summary(lr.fit)
summary(model_ns)
summary(model_gam)

anova(lr.fit)
anova(model_ns)
anova(model_gam)
```



```{r}
get_metrics<- function(y_true, y_pred) {
  rmse <- sqrt(mean((y_true - y_pred)^2))
  mape <- mean(abs((y_true - y_pred) / y_true)) * 100
  r2 <- 1 - sum((y_true - y_pred)^2) / sum((y_true - mean(y_true))^2)
  return(c(RMSE = rmse, MAPE = mape, R2 = r2))
}

metrics_price <- get_metrics(test$resale_price, pred.xgb)
metrics_price


```
```{r}
results <- data.frame()
train_linear <- df_linear[trainidx,]
test_linear <- df_linear[-trainidx,]
head(train_linear)
test_linear %>%
  filter(flat_type == "3 ROOM") -> test_sub1
test_linear %>%
  filter(flat_type == "4 ROOM") -> test_sub2
test_linear %>%
  filter(flat_type == "5 ROOM") -> test_sub3



train_tree <- df_tree[trainidx,]
test_tree <- df_tree[-trainidx,]
head(train_tree)

test_tree %>%
  filter(flat_type == "3 ROOM") -> test_sub1
test_tree %>%
  filter(flat_type == "4 ROOM") -> test_sub2
test_tree %>%
  filter(flat_type == "5 ROOM") -> test_sub3

```
```{r}
pred1_lr = predict(lr.fit, newdata=test_sub1)
pred2_lr = predict(lr.fit, newdata=test_sub2)
pred3_lr = predict(lr.fit, newdata=test_sub3)
pred1_ns = predict(model_ns, newdata=test_sub1)
pred2_ns = predict(model_ns, newdata=test_sub2)
pred3_ns = predict(model_ns, newdata=test_sub3)
pred1_gam = predict(model_gam, newdata=test_sub1)
pred2_gam = predict(model_gam, newdata=test_sub2)
pred3_gam = predict(model_gam, newdata=test_sub3)

rmse1_lr = sqrt(mean((test_sub1$resale_price - exp(pred1_lr))^2))
rmse2_lr = sqrt(mean((test_sub2$resale_price - exp(pred2_lr))^2))
rmse3_lr = sqrt(mean((test_sub3$resale_price - exp(pred3_lr))^2))
rmse1_ns = sqrt(mean((test_sub1$resale_price - exp(pred1_ns))^2))
rmse2_ns = sqrt(mean((test_sub2$resale_price - exp(pred2_ns))^2))
rmse3_ns = sqrt(mean((test_sub3$resale_price - exp(pred3_ns))^2))
rmse1_gam = sqrt(mean((test_sub1$resale_price - exp(pred1_gam))^2))
rmse2_gam = sqrt(mean((test_sub2$resale_price - exp(pred2_gam))^2))
rmse3_gam = sqrt(mean((test_sub3$resale_price - exp(pred3_gam))^2))

rmse_df <- data.frame(
  Model = c("lm.fit", "ns.fit", "gam.fit"),
  "3 ROOM" = c(rmse1_lr, rmse1_ns, rmse1_gam),
  "4 ROOM" = c(rmse2_lr, rmse2_ns, rmse2_gam),
  "5 ROOM" = c(rmse3_lr, rmse3_ns, rmse3_gam)
)

print(rmse_df)

pred1_rg = predict(pruned.hdb, newdata=test_sub1)
pred2_rg = predict(pruned.hdb, newdata=test_sub2)
pred3_rg = predict(pruned.hdb, newdata=test_sub3)
pred1_boost = predict(boost.tree, newdata=test_sub1)
pred2_boost = predict(boost.tree, newdata=test_sub2)
pred3_boost = predict(boost.tree, newdata=test_sub3)
X_test <- model.matrix(resale_price ~ . -1, data = test_sub1)
y_test <- test_sub1$resale_price 
dtest <- xgb.DMatrix(data = X_test)
pred1_xgb <- predict(xgb_model, newdata = dtest,  iteration_range = c(0, best_iter))
X_test <- model.matrix(resale_price ~ . -1, data = test_sub2)
y_test <- test_sub2$resale_price 
dtest <- xgb.DMatrix(data = X_test)
pred2_xgb <- predict(xgb_model, newdata = dtest,  iteration_range = c(0, best_iter))
X_test <- model.matrix(resale_price ~ . -1, data = test_sub3)
y_test <- test_sub3$resale_price 
dtest <- xgb.DMatrix(data = X_test)
pred3_xgb <- predict(xgb_model, newdata = dtest,  iteration_range = c(0, best_iter))

rmse1_rg = sqrt(mean((test_sub1$resale_price - pred1_rg)^2))
rmse2_rg = sqrt(mean((test_sub2$resale_price - pred2_rg)^2))
rmse3_rg = sqrt(mean((test_sub3$resale_price - pred3_rg)^2))
rmse1_boost = sqrt(mean((test_sub1$resale_price - pred1_boost)^2))
rmse2_boost = sqrt(mean((test_sub2$resale_price - pred2_boost)^2))
rmse3_boost = sqrt(mean((test_sub3$resale_price - pred3_boost)^2))
rmse1_xgb = sqrt(mean((test_sub1$resale_price - pred1_xgb)^2))
rmse2_xgb = sqrt(mean((test_sub2$resale_price - pred2_xgb)^2))
rmse3_xgb = sqrt(mean((test_sub3$resale_price - pred3_xgb)^2))

new_rmse_df <- data.frame(
  Model = c("rg.fit", "boost.fit", "xgb.fit"),
  "3 ROOM" = c(rmse1_rg, rmse1_boost, rmse1_xgb),
  "4 ROOM" = c(rmse2_rg, rmse2_boost, rmse2_xgb),
  "5 ROOM" = c(rmse3_rg, rmse3_boost, rmse3_xgb)
)

rmse_df <- rbind(rmse_df, new_rmse_df)
print(rmse_df)
```











